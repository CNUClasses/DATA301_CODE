{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random Forest regressor example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Set max rows and columns displayed in jupyter\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "#the following gives access to utils folder\n",
    "#where utils package stores shared code\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\n",
    "                  os.getcwd(),\n",
    "                  os.pardir)\n",
    ")\n",
    "\n",
    "#only add it once\n",
    "if (PROJECT_ROOT not in sys.path):\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generally a datascience problem starts with a question:\n",
    "1. I want to buy a house in a very safe neighborhood for as little as possible.\n",
    "2. I want to buy a house in a neighborhood that has the best chance of appreciation.\n",
    "\n",
    "Then you find data that will help you answer questions. For instance;\n",
    "1. You need crime records by zipcode/neighborhood and sales data, maybe generate a feature thats a combination of crime_score and average sales price. Output a list of safeest/lowest price neighborhoods\n",
    "2. You need historical sales data (how far back is relevant?), information on schools including an unbiased rating, probably walkability indexes, proximity to hospital, traffic info.  Probably stiching together a lot of separate data sources.\n",
    "\n",
    "We will do something a bit simpler here, just estimate a houses price based on a bunch of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data if you do not have it\n",
    "import os\n",
    " \n",
    "# Specify path\n",
    "path='../datasets1'\n",
    "file = '../datasets1/melb_data.csv'\n",
    "\n",
    "#ceate if not there\n",
    "if(not os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    " \n",
    "# Check whether file exists or not\n",
    "if( not os.path.exists(file)):    \n",
    "    import wget\n",
    "    # Define the remote file to retrieve\n",
    "    remote_url = 'https://raw.githubusercontent.com/CNUClasses/DATA301_CODE/master/datasets/melb_data.csv'\n",
    "    # Define the local filename to save data\n",
    "    local_file = file\n",
    "    # Make http request for remote file data\n",
    "    wget.download(remote_url, local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/melb_data.csv\")\n",
    "df.head()\n",
    "print(f'there are {len(df)} rows in df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTW if you are doing EDA or model building AND your dataset is large then work with a randomly sampled fraction of the data.\n",
    "\n",
    "This will hugely speed up EDA and model training.  You should do this every time when you first start working with a dataset, especially if it's a large one.\n",
    "\n",
    "<mark>When you finish tuning, finding hyperparameters and such, then use the FULL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a fraction (10%) of the data to work on\n",
    "# df=df.sample( frac=.1, replace=False, weights=None, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#get 10 rows to work on (used to demonstrate MAE below)\n",
    "# df=df.sample( n=10, replace=False, weights=None, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f'there are {len(df)} rows in df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets strip some columns to make this demo easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets strip out just a few columns to make this example easier\n",
    "# Choose target and features\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'Suburb','Postcode',\n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "\n",
    "#dependant variable (what you are estimating)\n",
    "y = df.Price\n",
    "\n",
    "#independant variables\n",
    "df = df[features]\n",
    "\n",
    "print(f'there are {len(df)} rows in df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what types are we dealing with\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: What can you tell by just plotting some stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    _=plt.figure()\n",
    "    _=sns.scatterplot(data=df,x=col,y=y, alpha=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above\n",
    "1. I see most of the houses have between 2 and 5 rooms \n",
    "2. I also see most have between 1 and 3 bathrooms\n",
    "\n",
    "My guess is the model will be best at predicting the price for houses 2-5 rooms and 1-3 bathrooms since there are so many examples for it to learn from\n",
    "\n",
    "3. Landsize has some outliers, I'm sure the model will not do well predicting on houses that have land sizes>5000 since there are so few examples\n",
    "\n",
    "4. Building area has outliers, either they are just wrong or its an apartment building mixed in with regular houses.  How will the model do here?\n",
    "\n",
    "5. One YearBuilt is 1200AD. This is clearly wrong. (How would you fix this? See bottom of page)\n",
    "6. Notice that column of YearBuilt values around  1975.  What do you think that means?\n",
    "7. Lattitude and longitude tell you where most of the houses are.  What do you think would happen if you clustered this data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess?\n",
    "Don't have to do much for random forest (convert strings to numbers) and handle nulls, treat all catagoricals as ordinal.  Dont worry about normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first are there any?\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nans(cols):\n",
    "    for col in cols:\n",
    "        def imputeval(df,targ_col,imput_col):\n",
    "            #get average values based on imput_col\n",
    "            sm=df.groupby(by=[imput_col])[targ_col].mean()\n",
    "\n",
    "            #for a given row, look up and return the mean col\n",
    "            def impute(x):\n",
    "                return sm.loc[x[imput_col]]\n",
    "\n",
    "            #replace NaNs with the average\n",
    "            df[targ_col]=df.apply(lambda x:impute(x) if np.isnan(x[targ_col]) else x[targ_col],axis=1)\n",
    "\n",
    "        imputeval(df,col,'Suburb')\n",
    "        imputeval(df,col,'Postcode')\n",
    "        #if here probably no good way to impute the remainders\n",
    "        # dont want to drop them so set them to the column mean\n",
    "        df[col]=df[col].fillna(df[col].mean())\n",
    "\n",
    "fix_nans(cols=['YearBuilt','BuildingArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the outliers to get better plots for Yearbuilt, BuildingArea and Landsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and  set the bogus values to NaNs\n",
    "df.loc[df['YearBuilt']<1700,'YearBuilt']=np.NaN\n",
    "df.loc[df['BuildingArea']>1000,'BuildingArea']=np.NaN\n",
    "df.loc[df['Landsize']>10000,'Landsize']=np.NaN\n",
    "\n",
    "fix_nans(cols=['YearBuilt','Landsize','BuildingArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont need Postcode\n",
    "df.drop(columns=['Postcode'], inplace=True)\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea','Suburb' ,\n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suburb probably captures excellent info about value, encode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils.cat_ordinal would work fine with a mapping\n",
    "#so generate a mapping\n",
    "suburb_map={i:v for v,i in enumerate(list(df.Suburb.unique()))}\n",
    "import utils as ut\n",
    "order={'Suburb':suburb_map}\n",
    "df=ut.cat_ordinal(df,['Suburb'],order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "How are we going to see if we are right?  We cannot use accuracy here like we did for classification since we are predicting a floating point number, and even if 2 floating point numbers are very close, in a boolean expression they will not be equal (ex 0.002 != 0.002001).\n",
    "\n",
    "So what to do?  Measure the difference between the true value and the predicted value or each point, sum these differences, and then divide by the number of points.  This is called the <mark>mean_absolute_error</mark> and it's given by the formula below.\n",
    "![](./mae_form.png)\n",
    "\n",
    "A MAE of 0 means your model is perfect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(df, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BTW mean absolute error is simple, here is an implementation\n",
    "def mae(val_y, predst):\n",
    "    tots=0\n",
    "    for y,p in zip(val_y, predst):\n",
    "        tots=tots +np.abs(p-y)\n",
    "    return tots/len(val_y)\n",
    "\n",
    "#but you are better off using sklearns implementation (less code to write)\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the average house price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision tree Regressor (remember this is a single tree)\n",
    "\n",
    "Scikitlearn estimators work the same way,;\n",
    "1. Fit estimator to the training data\n",
    "2. Generate predictions on validation data\n",
    "3. Use an error metric to determine how good model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Fit estimator to the training data\n",
    "\n",
    "\n",
    "# 2. Generate predictions on validation data\n",
    "\n",
    "\n",
    "# 3. Use an error metric to determine how good model is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that home rolled mae is the same as sklearns mean_absolute_error\n",
    "# print(f'mae error={mae(val_y, predst)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then train a random forest regressor (lots of decision trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word about OOB error\n",
    "\n",
    "You can use OOB score but scikitlearn RandomForestRegressor hardcodes the error metric to r2_score and  we want to use MAE so we will forgo OOB here.\n",
    "You can see this hardcoding by creating the RandomForestRegressor and then looking at the source code for _set_oob_score_and_attributes\n",
    "<code> \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_model = RandomForestRegressor(random_state=42, oob_score=True)\n",
    "forest_model._set_oob_score_and_attributes??\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Fit estimator to the training data\n",
    "\n",
    "# 2. Generate predictions on validation data\n",
    "\n",
    "\n",
    "# 3. Use an error metric to determine how good model is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean absolute error for the random forest is considerably less than the decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember what I said above about how I thought the model would do well on houses with 2-5 rooms and 1-3 bathrooms since there are so many examples for it to learn from? Lets see if thats true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    _=plt.figure()\n",
    "    # _=sns.lineplot(data=val_X,x=col,y=val_y, alpha=1,errorbar=None,palette=['red'])\n",
    "    # _=sns.lineplot(data=val_X,x=col,y=preds, alpha=1,errorbar=None,palette=['green'])\n",
    "    _=sns.lineplot(data=val_X,x=col,y=val_y, alpha=1,errorbar=None,color='red')\n",
    "    _=sns.lineplot(data=val_X,x=col,y=preds, alpha=1,errorbar=None,color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
