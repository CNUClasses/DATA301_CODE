{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted trees verses random forest\n",
    "\n",
    "\n",
    "Compare LightGBM and catbost with sklearn's Random Forest<br>\n",
    "LightGBM is a microsoft gradient boosted tree product<br>\n",
    "catboost is a Yandex gradient boosted tree product<br>\n",
    "\n",
    "See <a href=\"https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db\">CatBoost vs. Light GBM vs. XGBoost</a> for relative comparisons<br>\n",
    "See <a href=\"https://towardsdatascience.com/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956\">Scikit-Learn vs XGBoost vs LightGBM vs CatBoost in Sentiment Classification</a> for another relative comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install LightGBM\n",
    "conda and pip both have the same version (3.3.2) and the last upload was 3 months ago. This package is currently being maintained. Prefer conda so anaconda can coordinate LightGBMs dependencies with all other conda packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !conda install -c conda-forge lightgbm -y\n",
    "import lightgbm\n",
    "lightgbm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install catboost\n",
    "conda and pip both have the same version (1.0.4) and the last upload was 2 months ago. This package is currently being maintained. Prefer conda so anaconda can coordinate catboosts dependencies with all other conda packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.6'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !conda install -c conda-forge catboost -y\n",
    "import catboost\n",
    "catboost.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "calif_housing = fetch_california_housing()\n",
    "\n",
    "# for line in calif_housing.DESCR.split(\"\\n\")[5:22]:\n",
    "#     print(line)\n",
    "\n",
    "calif_housing_df = pd.DataFrame(data=calif_housing.data, columns=calif_housing.feature_names)\n",
    "calif_housing_df[\"Price($)\"] = calif_housing.target\n",
    "\n",
    "# calif_housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = calif_housing.data, calif_housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R squared - a way to qualify a models predictions\n",
    "\n",
    "The following regressors use R squared as the default objective to optimize.  See <a href=\"https://www.youtube.com/watch?v=2AQKmw14mHM\">Statquest: R-squared, Clearly Explained!!!</a> for a great explanation plus examples.\n",
    "\n",
    "Usually 0<R squared<1  .  It ranges between these 2 values and is interpreted as how well the model fits the data. (In statistics this is called explained variance)\n",
    "\n",
    "If R squared =0,the line fitted to data is no more accurate than taking the mean of the data.<br>\n",
    "If R squared =1,the line fitted to the data is a perfect match<br>\n",
    "If R squared is negative then the line fitted to the data is a worse fit than just taking the average value of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It was not clear what objective lightGBM optimizes\n",
    "#so I implemented R squared below \n",
    "def rsquared(preds, y):\n",
    "    RSS=np.sum(np.square(preds-y))\n",
    "    ymean=np.sum(y)/len(y)\n",
    "    TSS=np.sum(np.square(y-ymean))\n",
    "    return 1-RSS/TSS\n",
    "\n",
    "def scoremodel(clf,X_test, y_test):\n",
    "    print(\"Score on test set: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "    #run score using rsquared function above\n",
    "    preds=clf.predict(X_test)\n",
    "    rsq=rsquared(preds,y_test)\n",
    "    print(\"Score on test set using rsquared: {:.2f}\".format(rsq))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest- default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 36.8 ms, total: 11.8 s\n",
      "Wall time: 908 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#random forest can be done in parallel, set n_jobs=-1 to use all processors\n",
    "clf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "_=clf.fit(X_train, y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.81\n",
      "Score on test set using rsquared: 0.81\n"
     ]
    }
   ],
   "source": [
    "scoremodel(clf,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm- default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 44.3 ms, total: 3.1 s\n",
      "Wall time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMRegressor\n",
    "clf = LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "_=clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.84\n",
      "Score on test set using rsquared: 0.84\n"
     ]
    }
   ],
   "source": [
    "scoremodel(clf,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost -default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 1.07 s, total: 15.5 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from catboost import CatBoostRegressor\n",
    "clf = CatBoostRegressor(silent=True, random_state=42)\n",
    "\n",
    "_=clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.86\n",
      "Score on test set using rsquared: 0.86\n"
     ]
    }
   ],
   "source": [
    "scoremodel(clf,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan_mode': 'Min',\n",
       " 'eval_metric': 'RMSE',\n",
       " 'iterations': 1000,\n",
       " 'sampling_frequency': 'PerTree',\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'grow_policy': 'SymmetricTree',\n",
       " 'penalties_coefficient': 1,\n",
       " 'boosting_type': 'Plain',\n",
       " 'model_shrink_mode': 'Constant',\n",
       " 'feature_border_type': 'GreedyLogSum',\n",
       " 'bayesian_matrix_reg': 0.10000000149011612,\n",
       " 'force_unit_auto_pair_weights': False,\n",
       " 'l2_leaf_reg': 3,\n",
       " 'random_strength': 1,\n",
       " 'rsm': 1,\n",
       " 'boost_from_average': True,\n",
       " 'model_size_reg': 0.5,\n",
       " 'pool_metainfo_options': {'tags': {}},\n",
       " 'subsample': 0.800000011920929,\n",
       " 'use_best_model': False,\n",
       " 'random_seed': 42,\n",
       " 'depth': 6,\n",
       " 'posterior_sampling': False,\n",
       " 'border_count': 254,\n",
       " 'classes_count': 0,\n",
       " 'auto_class_weights': 'None',\n",
       " 'sparse_features_conflict_fraction': 0,\n",
       " 'leaf_estimation_backtracking': 'AnyImprovement',\n",
       " 'best_model_min_trees': 1,\n",
       " 'model_shrink_rate': 0,\n",
       " 'min_data_in_leaf': 1,\n",
       " 'loss_function': 'RMSE',\n",
       " 'learning_rate': 0.0637660026550293,\n",
       " 'score_function': 'Cosine',\n",
       " 'task_type': 'CPU',\n",
       " 'leaf_estimation_iterations': 1,\n",
       " 'bootstrap_type': 'MVS',\n",
       " 'max_leaves': 64}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_all_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters for catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 01:45:16,775]\u001b[0m A new study created in memory with name: no-name-5f5321d5-3b5c-4cc9-a169-86cea1f11ad9\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:45:18,569]\u001b[0m Trial 0 finished with value: 0.18354207895951047 and parameters: {'learning_rate': 0.081, 'depth': 7, 'l2_leaf_reg': 2.5}. Best is trial 0 with value: 0.18354207895951047.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:45:19,933]\u001b[0m Trial 1 finished with value: 0.23510875876300527 and parameters: {'learning_rate': 0.012, 'depth': 6, 'l2_leaf_reg': 3.5}. Best is trial 0 with value: 0.18354207895951047.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:45:20,850]\u001b[0m Trial 2 finished with value: 0.2117854546006257 and parameters: {'learning_rate': 0.081, 'depth': 3, 'l2_leaf_reg': 1.5}. Best is trial 0 with value: 0.18354207895951047.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:45:38,672]\u001b[0m Trial 3 finished with value: 0.20630553313630487 and parameters: {'learning_rate': 0.013000000000000001, 'depth': 11, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: 0.18354207895951047.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:45:39,842]\u001b[0m Trial 4 finished with value: 0.18847547665888378 and parameters: {'learning_rate': 0.089, 'depth': 5, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: 0.18354207895951047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 0.18354207895951047\n",
      "Best hyperparameters: {'learning_rate': 0.081, 'depth': 7, 'l2_leaf_reg': 2.5}\n",
      "CPU times: user 2min 53s, sys: 7.74 s, total: 3min 1s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "import sklearn\n",
    "def objective(trial):\n",
    "    #these are the parameters I want to optimize\n",
    "    params = {\n",
    "        'learning_rate':trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.09, 0.001),\n",
    "        'depth': trial.suggest_int(\"depth\", 1, 12),\n",
    "        'l2_leaf_reg':trial.suggest_discrete_uniform('l2_leaf_reg', 1.0, 5.5, 0.5),\n",
    "        'iterations':1000,\n",
    "        'silent':True,\n",
    "        'random_state':42\n",
    "    }\n",
    " \n",
    "    # Define the model. Pass in params to be tuned\n",
    "    clf = CatBoostRegressor(**params)\n",
    "    \n",
    "    clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)\n",
    "\n",
    "    return mean_squared_error(y_test, clf.predict(X_test))\n",
    " \n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('mean_squared_error: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 01:47:28,959]\u001b[0m Trial 5 finished with value: 0.1808923330294024 and parameters: {'learning_rate': 0.081, 'depth': 9, 'l2_leaf_reg': 1.5}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:47:30,083]\u001b[0m Trial 6 finished with value: 0.3294701737957197 and parameters: {'learning_rate': 0.004, 'depth': 5, 'l2_leaf_reg': 1.0}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:47:47,962]\u001b[0m Trial 7 finished with value: 0.1927326417397994 and parameters: {'learning_rate': 0.025, 'depth': 11, 'l2_leaf_reg': 4.0}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:47:48,650]\u001b[0m Trial 8 finished with value: 0.33344312711302915 and parameters: {'learning_rate': 0.061, 'depth': 1, 'l2_leaf_reg': 1.5}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:47:50,415]\u001b[0m Trial 9 finished with value: 0.18579053458364106 and parameters: {'learning_rate': 0.055, 'depth': 7, 'l2_leaf_reg': 2.0}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:47:55,063]\u001b[0m Trial 10 finished with value: 0.18621397364846293 and parameters: {'learning_rate': 0.041, 'depth': 9, 'l2_leaf_reg': 5.5}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:47:57,608]\u001b[0m Trial 11 finished with value: 0.18165726721639805 and parameters: {'learning_rate': 0.074, 'depth': 8, 'l2_leaf_reg': 2.5}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:48:02,181]\u001b[0m Trial 12 finished with value: 0.18378105913203166 and parameters: {'learning_rate': 0.066, 'depth': 9, 'l2_leaf_reg': 4.5}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:48:06,750]\u001b[0m Trial 13 finished with value: 0.18111419120082375 and parameters: {'learning_rate': 0.07100000000000001, 'depth': 9, 'l2_leaf_reg': 2.0}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:48:40,462]\u001b[0m Trial 14 finished with value: 0.18804954405521127 and parameters: {'learning_rate': 0.043000000000000003, 'depth': 12, 'l2_leaf_reg': 1.0}. Best is trial 5 with value: 0.1808923330294024.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#not satisfied?  Keep on optimizing from where you left off above\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost -using best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.86\n",
      "Score on test set using rsquared: 0.86\n",
      "CPU times: user 18.2 s, sys: 1.36 s, total: 19.5 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = CatBoostRegressor(**trial.params,silent=True, random_state=42)\n",
    "_=clf.fit(X_train, y_train)\n",
    "\n",
    "scoremodel(clf,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Optimize parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 01:03:33,650]\u001b[0m A new study created in memory with name: no-name-0fd993d5-ede0-4f03-a89a-67e6451e3d7d\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:38,455]\u001b[0m Trial 0 finished with value: -0.4862874292542784 and parameters: {'n_estimators': 92, 'max_depth': 54, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 0 with value: -0.4862874292542784.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:39,897]\u001b[0m Trial 1 finished with value: -0.4847104603070296 and parameters: {'n_estimators': 24, 'max_depth': 40, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 1 with value: -0.4847104603070296.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:45,479]\u001b[0m Trial 2 finished with value: -0.4854579401085435 and parameters: {'n_estimators': 122, 'max_depth': 33, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 1 with value: -0.4847104603070296.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:46,505]\u001b[0m Trial 3 finished with value: -0.48425659389633574 and parameters: {'n_estimators': 16, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 3 with value: -0.48425659389633574.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:51,043]\u001b[0m Trial 4 finished with value: -0.48020437615531525 and parameters: {'n_estimators': 82, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 4 with value: -0.48020437615531525.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:54,905]\u001b[0m Trial 5 finished with value: -0.4799994981807118 and parameters: {'n_estimators': 67, 'max_depth': 22, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 5 with value: -0.4799994981807118.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:03:58,311]\u001b[0m Trial 6 finished with value: -0.4881544400471011 and parameters: {'n_estimators': 75, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 5 with value: -0.4799994981807118.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:04:02,268]\u001b[0m Trial 7 finished with value: -0.4858082365892123 and parameters: {'n_estimators': 87, 'max_depth': 39, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 5 with value: -0.4799994981807118.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:04:03,916]\u001b[0m Trial 8 finished with value: -0.5401139704377523 and parameters: {'n_estimators': 68, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 5 with value: -0.4799994981807118.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 01:04:04,348]\u001b[0m Trial 9 finished with value: -0.5842776621870234 and parameters: {'n_estimators': 23, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 5 with value: -0.4799994981807118.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_mean_absolute_error: -0.4799994981807118\n",
      "Best hyperparameters: {'n_estimators': 67, 'max_depth': 22, 'min_samples_split': 9, 'min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import optuna\n",
    "import sklearn\n",
    "def objective(trial,X=X,y=y):\n",
    "    \n",
    "    params = {'depth': [4, 7, 10],\n",
    "          'learning_rate' : [0.03, 0.1, 0.15],\n",
    "         'l2_leaf_reg': [1,4,9],\n",
    "         'iterations': [300]}\n",
    "    \n",
    "    #these are the parameters I want to optimize\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 130)\n",
    "    max_depth = int(trial.suggest_int('max_depth', 1, 55))\n",
    "    min_samples_split = int(trial.suggest_int('min_samples_split', 2, 10))\n",
    "    min_samples_leaf= int(trial.suggest_int('min_samples_leaf', 1, 10))\n",
    "\n",
    "    # Define the model. Pass in params to be tuned\n",
    "    clf = RandomForestRegressor(random_state=42, n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf )  \n",
    "\n",
    "    #get the cross validation score\n",
    "    return sklearn.model_selection.cross_val_score( clf, X, y, n_jobs=-1, cv=3, scoring='neg_mean_absolute_error').mean() \n",
    " \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('neg_mean_absolute_error: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>103</td>\n",
       "      <td>MKE</td>\n",
       "      <td>DCA</td>\n",
       "      <td>102.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>634</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>B6</td>\n",
       "      <td>153</td>\n",
       "      <td>PBI</td>\n",
       "      <td>JFK</td>\n",
       "      <td>134.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1028</td>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>DL</td>\n",
       "      <td>1187</td>\n",
       "      <td>DCA</td>\n",
       "      <td>MSP</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>931</td>\n",
       "      <td>-19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>WN</td>\n",
       "      <td>171</td>\n",
       "      <td>RDU</td>\n",
       "      <td>DEN</td>\n",
       "      <td>173.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>WN</td>\n",
       "      <td>4330</td>\n",
       "      <td>RIC</td>\n",
       "      <td>ATL</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2151.0</td>\n",
       "      <td>481</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER DESTINATION_AIRPORT  \\\n",
       "0      1   28            3      WN            103                 MKE   \n",
       "1      8   11            2      B6            153                 PBI   \n",
       "2      2    4            3      DL           1187                 DCA   \n",
       "3      3   27            5      WN            171                 RDU   \n",
       "4      8    1            6      WN           4330                 RIC   \n",
       "\n",
       "  ORIGIN_AIRPORT  AIR_TIME  DEPARTURE_TIME  DISTANCE  ARRIVAL_DELAY  \n",
       "0            DCA     102.0           713.0       634            1.0  \n",
       "1            JFK     134.0           111.0      1028          337.0  \n",
       "2            MSP     111.0          1734.0       931          -19.0  \n",
       "3            DEN     173.0          1807.0      1436           -7.0  \n",
       "4            ATL      63.0          2151.0       481           13.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took 10% of original dataset, dropped bunch of columns\n",
    "data = pd.read_csv(\"../datasets/kaggle/flights/flights_small.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MONTH                    12\n",
       "DAY                      31\n",
       "DAY_OF_WEEK               7\n",
       "AIRLINE                  14\n",
       "FLIGHT_NUMBER          6688\n",
       "DESTINATION_AIRPORT     624\n",
       "ORIGIN_AIRPORT          623\n",
       "AIR_TIME                613\n",
       "DEPARTURE_TIME         1414\n",
       "DISTANCE               1324\n",
       "ARRIVAL_DELAY           737\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only later than 10 minutes is considered late\n",
    "data[\"ARRIVAL_DELAY\"] = (data[\"ARRIVAL_DELAY\"]>10)*1\n",
    "\n",
    "#convert to ordinal (even though they are categorical)\n",
    "cols = [\"AIRLINE\",\"FLIGHT_NUMBER\",\"DESTINATION_AIRPORT\",\"ORIGIN_AIRPORT\"]\n",
    "for item in cols:\n",
    "    data[item] = data[item].astype(\"category\").cat.codes +1\n",
    "\n",
    "#unlbalanced dataset, make sure you get a stratified sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"ARRIVAL_DELAY\"], axis=1), data[\"ARRIVAL_DELAY\"], stratify=data[\"ARRIVAL_DELAY\"],\n",
    "                                                random_state=10, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    111559\n",
       "1     31276\n",
       "Name: ARRIVAL_DELAY, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.head()\n",
    "# X_test.shape\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def score_classifier_model(clf,X_test, y_test):\n",
    "    res = clf.predict(X_test)\n",
    "    print (classification_report(y_test, res))\n",
    "    print(\"And the confusion matrix\")\n",
    "    print(confusion_matrix(y_test,res))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest- default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 31s, sys: 637 ms, total: 3min 31s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfc = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "_=clfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89    111559\n",
      "           1       0.71      0.17      0.28     31276\n",
      "\n",
      "    accuracy                           0.80    142835\n",
      "   macro avg       0.76      0.58      0.58    142835\n",
      "weighted avg       0.79      0.80      0.75    142835\n",
      "\n",
      "And the confusion matrix\n",
      "[[109344   2215]\n",
      " [ 25809   5467]]\n"
     ]
    }
   ],
   "source": [
    "score_classifier_model(clfc,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro avg (precision)   =(.81+.71)/2\n",
    "# weighted avg (precision)= (111559/142835)*.81 + ((31276/142835)*.71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm- default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 58.1 ms, total: 10.1 s\n",
      "Wall time: 699 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "clf_lgbm = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "_=clf_lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88    111559\n",
      "           1       0.74      0.12      0.20     31276\n",
      "\n",
      "    accuracy                           0.80    142835\n",
      "   macro avg       0.77      0.55      0.54    142835\n",
      "weighted avg       0.79      0.80      0.73    142835\n",
      "\n",
      "And the confusion matrix\n",
      "[[110268   1291]\n",
      " [ 27610   3666]]\n"
     ]
    }
   ],
   "source": [
    "score_classifier_model(clf_lgbm,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost -default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 37s, sys: 4.48 s, total: 3min 41s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from catboost import CatBoostClassifier\n",
    "clf_catboost = CatBoostClassifier(silent=True, random_state=42)\n",
    "_=clf_catboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89    111559\n",
      "           1       0.72      0.19      0.31     31276\n",
      "\n",
      "    accuracy                           0.81    142835\n",
      "   macro avg       0.77      0.59      0.60    142835\n",
      "weighted avg       0.79      0.81      0.76    142835\n",
      "\n",
      "And the confusion matrix\n",
      "[[109215   2344]\n",
      " [ 25226   6050]]\n"
     ]
    }
   ],
   "source": [
    "score_classifier_model(clf_catboost,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice that catboost outperforms Random Forest and LightGBM for these 2 tasks?  See the articles mentioned in first cell for further evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
