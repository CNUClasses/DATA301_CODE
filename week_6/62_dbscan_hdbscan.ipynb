{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBscan and HDBscan with KMeans as a baseline\n",
    "\n",
    "DBscan example derived from <a href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py\">Demo of DBSCAN clustering algorithm</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kneed\n",
    "#!pip install hdbscan\n",
    "\n",
    "#want to filter the seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import sklearn.datasets as data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Set max rows and columns displayed in jupyter\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "#the following gives access to utils folder\n",
    "#where utils package stores shared code\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\n",
    "                  os.getcwd(),\n",
    "                  os.pardir)\n",
    ")\n",
    "\n",
    "#only add it once\n",
    "if (PROJECT_ROOT not in sys.path):\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    \n",
    "import utils as ut\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSAMPLES=300\n",
    "RANDOM_STATE=999\n",
    "MIN_SAMPLES = 6\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def get_sorted_distances( X, min_samples=MIN_SAMPLES):\n",
    "    '''\n",
    "    X:data \n",
    "    distances: the sorted distances to MIN_SAMPLES points for every point in X\n",
    "    plot: to plot the knee or not\n",
    "    return: sorted distances in descending order\n",
    "    '''\n",
    "    nbrs = NearestNeighbors(n_neighbors=min_samples ).fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "    distance_descending = sorted(distances[:,min_samples-1], reverse=True)\n",
    "    return distance_descending\n",
    "\n",
    "\n",
    "#kneed is not part of scikitlearn or a standard anaconda distribution, here is how to install\n",
    "# !conda install -c conda-forge kneed -y\n",
    "from kneed import KneeLocator\n",
    "def get_eps(X,distances, plot=False):\n",
    "    '''\n",
    "    X:data \n",
    "    distances: the sorted distances toMIN_SAMPLES points for every point in X\n",
    "    plot: to plot the knee or not\n",
    "    return: eps\n",
    "    '''\n",
    "    kneedle = KneeLocator(range(1,len(X)+1),  #x values\n",
    "                      distances, # y values\n",
    "                      S=1.0, #sensitivity\n",
    "                      curve=\"convex\", \n",
    "                      direction=\"decreasing\") #parameter from figure\n",
    "    if(plot):\n",
    "        kneedle.plot_knee()\n",
    "    eps=kneedle.knee_y  # optimum value for eps\n",
    "    # kneedle.elbow\n",
    "    # kneedle.knee\n",
    "    return eps\n",
    "\n",
    "\n",
    "def display_cluster_info(fitted_estimator,X, misclassified_indices=None):\n",
    "    '''\n",
    "    fitted_estimator: an estimator that has been fitted to data\n",
    "    X: data that estimator was fitted to\n",
    "    return: number of clusters and cluster_labels\n",
    "    '''\n",
    "    #cluster labels for dataset\n",
    "    cluster_labels = fitted_estimator.labels_\n",
    "\n",
    "    if(misclassified_indices is not None):\n",
    "        cluster_labels[misclassified_indices] = -2\n",
    "        print(\"Estimated number of misclassified indices: %d\" % len(misclassified_indices))\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise_ = list(cluster_labels).count(-1)\n",
    "\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "    sns.scatterplot(x=X[:,0], y=X[:,1],hue=cluster_labels, palette=ut.colors1)\n",
    " \n",
    "    return n_clusters,cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this generates a dataset with blobs and moons\n",
    "moons, _ = data.make_moons(n_samples=NSAMPLES, noise=0.05)\n",
    "blobs, _ = data.make_blobs(n_samples=NSAMPLES, centers=[(-0.75,2.25), (1.0, 2.0)], cluster_std=0.3)\n",
    "X = np.vstack([moons, blobs])\n",
    "plot_kwds = {'alpha' : 0.5, 's' : 20, 'linewidths':0}\n",
    "plt.scatter(X.T[0], X.T[1], color='b', **plot_kwds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans (baseline performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=RANDOM_STATE).fit(X)\n",
    "n_clusters,cluster_labels=display_cluster_info(kmeans,X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBscan\n",
    "\n",
    "Parameters of Interest:\n",
    "    \n",
    "   <u>eps (radius)</u>- The maximum distance between two samples for one to be considered in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set since it determines density (ie smaller eps means core points must have surrounding points closer in order to be a core point) <br>\n",
    "    <u>min_samples</u> - The minimum number of samples within eps of a point for that point to be considered a core point. This includes the point itself.\n",
    "    \n",
    "## Guess min_samples\n",
    "Its hard to choose this one. Without specialized knowledge, arbitrary rule of thumb is to choose 2*(numb_features+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find a good eps for above min_samples\n",
    "We do this in 2 steps, first decide on min_samples (number of sample) <br>\n",
    "1. Calculate the distances to MIN_SAMPLES for every point in the dataset, then sort the results in descending order. Some of these points will be in a high density area (core and non-core points), so the average MIN_SAMPLES distances will be smaller, others will be in a low density area (outliers), so the average MIN_SAMPLES will be larger<br>\n",
    "2. Find where the average distances starts growing the fastest.  Draw a vertical line through this point.  Any eps to the left is a distance large enough that it risks erroneously merging distinct clusters together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "Eyeball the line graph below, find point of maximum curvature, eps= the value of Y at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to walk thru the calcs\n",
    "# X1=X[:10,:]\n",
    "# dd=get_sorted_distances(X1,3)\n",
    "# px.line(x=list(range(1,len(dd )+1)),y= dd,labels={'x':'Points','y':'Distance'},title='Knee Point')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "distance_descending=get_sorted_distances( X,min_samples=MIN_SAMPLES)\n",
    "px.line(x=list(range(1,len(distance_descending )+1)),y= distance_descending,labels={'x':'Points','y':'Distance'},title='Knee Point')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Automate eps search with kneed\n",
    "Parameters defined <a href=\"https://kneed.readthedocs.io/en/stable/parameters.html#curve\">here</a><br>\n",
    "Given x and y arrays, kneed attempts to identify the knee/elbow point of a line fit to the data. <mark>The knee/elbow is defined as the point of the line with maximum curvature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_descending=get_sorted_distances( X,min_samples=MIN_SAMPLES)\n",
    "eps=get_eps(X,distance_descending, plot=True)\n",
    "print(f'The best eps={eps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm \n",
    "Have min_samples and a calculated eps that works well for this dataset and min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_descending=get_sorted_distances( X,min_samples=MIN_SAMPLES)\n",
    "eps=get_eps(X,distance_descending)\n",
    "db = DBSCAN(eps=eps, min_samples=MIN_SAMPLES).fit(X)\n",
    "n_clusters,cluster_labels=display_cluster_info(db,X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdbscan is not part of scikitlearn or a standard anaconda distribution, here is how to install\n",
    "# !conda install -c conda-forge hdbscan -y\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "#whats a good minimum cluster size? Another hyperparameter to guess\n",
    "MIN_CLUSTER_SIZE=125\n",
    "db = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE, min_samples=MIN_SAMPLES).fit(X)\n",
    "n_clusters,cluster_labels=display_cluster_info(db,X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sounds great right, try it with overlapping centers (), In section 3 above change cluster_std from .4 to .5\n",
    "\n",
    "This makes it so cluster points will get a little closer to each other. DBscan just lumps everything into 1 cluster, HDBscan does a bit better, but we do have a  bunch of outliers.<br>\n",
    "The moral: prefer HDBscan over DBscan. When there is cluster overlap both algorithms start to lose the ability to distinguish between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check the silhouette score for each algorithm\n",
    "Run the above algorithm for both DBscan and HDBscan and see what the silhouette scores are\n",
    "BTW these are very useful when you have high dimensional data so that you cant plot and visually confirm cluster membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1 but in this example all\n",
    "# lie within [-0.1, 1]\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "#find the indices where sil score is <=0 use this to go back and label possibly misclassified points\n",
    "misclassified_indices = np.where(sample_silhouette_values <= 0)[0]\n",
    "# Remove indices in misclassified_indices that are set to -1 in cluster_labels\n",
    "misclassified_indices = misclassified_indices[cluster_labels[misclassified_indices] != -1]\n",
    "#noise points are going to have sil score <=0, but dont indicate they are misclassified\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i));\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "    \n",
    "    ax1.set_title(\"The silhouette plot \");\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\");\n",
    "    ax1.set_ylabel(\"Cluster label\");\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\");\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cluster_info(db,X, misclassified_indices);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
