{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 63_inclass_cluster_example\n",
    "\n",
    "Preprocess data, a bit of feature engineering, PCA, clustering and then plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Set max rows and columns displayed in jupyter\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "#the following gives access to utils folder\n",
    "#where utils package stores shared code\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\n",
    "                  os.getcwd(),\n",
    "                  os.pardir)\n",
    ")\n",
    "\n",
    "#only add it once\n",
    "if (PROJECT_ROOT not in sys.path):\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Functions\n",
    "Migrate these to a separate package when done so they can be used with scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_NEW=\"Mall_CustomersNew.csv\"\n",
    "RSEED=7\n",
    "\n",
    "#lets compare each feature to all other features\n",
    "def show_pairplot(df, hue):\n",
    "    '''\n",
    "    Shows a pairplot comparing all the features in a dataframe\n",
    "    '''\n",
    "    #this works on a small dataset, will be prohibitivly slow on a larger one\n",
    "    To_Plot = [ col for col in df.columns]\n",
    "    print(\"Relative Plot Of Some Selected Features: A Data Subset\")\n",
    "    plt.figure()\n",
    "    sns.pairplot(data=df[To_Plot], hue=hue, palette=ut.colors1)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "df= pd.read_csv(F_NEW)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sns.pairplot to see if anything looks funny\n",
    "In this case Active has no variance and CustomerID is weirdly correlated with Annual Income.  It appears that they collected all customer data, sorted by income, and then assigned a customer ID to the sorted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show_pairplot(df,hue='Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transforms import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ut.remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle nulls\n",
    "Just drop if there are not too many and you can't figure out how to estimate<br>\n",
    "(You could estimate by taking the value before and after since the data was sorted by income before CustomerIDs were assigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many rows have nulls?\n",
    "df.isna().sum(axis=1).sum()\n",
    "\n",
    "#see em\n",
    "# df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only 1 null row, drop it\n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birthday-convert to Age in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert birthday to age in years\n",
    "import datetime\n",
    "def getyear(v):\n",
    "    return datetime.datetime.now().year - pd.to_datetime(v).year\n",
    "df['Birthday']=df.Birthday.map(getyear) \n",
    "df.rename(columns={\"Birthday\": \"Age\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations?\n",
    "Careful you want to drop the columns with the least info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any correlations?\n",
    "ut.get_correlated_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, y='Annual Income (k$)',x='CustomerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annual Income is highly (and weirdly) correlated with CustomerID, drop one.  But be sure to keep the one that has the most information!\n",
    "\n",
    "Looks like they sorted the dataset by incone then assigned consecutive IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomerID is likely unique for every customer, and contains no info, drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {df.CustomerID.nunique()} unique customer IDs and {len(df)} rows in df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['CustomerID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop no variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ut.drop_no_variance_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering- combine all spending columns into 1.  This assummes one value can accurately capture spending patterns. This also reduces number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets combine the last 3 into 1\n",
    "def combine_columns(df, newcolname, cols ):\n",
    "    '''\n",
    "    df: dataframe\n",
    "    newcolname: the name of the column to create that has the sum of all columns in cols\n",
    "    cols: list of columns to add\n",
    "    return: modded dataframe\n",
    "    '''\n",
    "    df[newcolname] = df[cols].sum(axis=1)\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# cols=[col for col in df.columns if \"spending\" in col]\n",
    "# data=combine_columns(df,\"spending_total\",cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#find categoricals, use df.dtypes, look for the object columns\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like Gender and Birthday\n",
    "print (df.Gender.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender is nominal, but it is binary ie male or female\n",
    "#try a binary variable instead of dummies\n",
    "feats=['Gender']\n",
    "\n",
    "#either of these\n",
    "#convert Gender\n",
    "# d= {v:i for i,v in enumerate(df.Gender.unique().tolist())}\n",
    "# df.Gender=df.Gender.map(d)\n",
    "df=ut.cat_ordinal(df, features=feats, order={'Gender':{'Male':0, 'Female':1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a copy of dataframe to append clusters to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dforig=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=ut.scale(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=.95, whiten=True)\n",
    "features_pca=pd.DataFrame(pca.fit_transform(df))\n",
    "print(f'Orig #features={df.shape[1]}, number features containing 95% of variance={features_pca.shape[1]}')\n",
    "\n",
    "features_pca\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdbscan is not part of scikitlearn or a standard anaconda distribution, here is how to install\n",
    "# !conda install -c conda-forge hdbscan -y\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "class hdbres():\n",
    "    '''\n",
    "    Bookkeeping class to hold data\n",
    "    '''\n",
    "    def __init__(self,min_cluster_size,min_samples,n_clusters,n_noise,labels ):\n",
    "        self.min_cluster_size=min_cluster_size\n",
    "        self.min_samples=min_samples\n",
    "        self.n_clusters=n_clusters\n",
    "        self.n_noise=n_noise\n",
    "        self.labels=labels  #all the cluster labels for my data\n",
    "    def __repr__(self):\n",
    "        return str(f'self.min_cluster_size:{self.min_cluster_size},min_samples:{self.min_samples}, numb clusters:{self.n_clusters},noise points: {self.n_noise}\\n')\n",
    "\n",
    "\n",
    "def run_hdbscan(df, min_cluster_size,min_samples, verbose=True):\n",
    "    '''\n",
    "    a hdbscan run for a set of parameters\n",
    "    returns: hdbres object with all initializing params and results\n",
    "    '''\n",
    "    db = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples).fit(df)\n",
    "\n",
    "    #cluster labels for dataset\n",
    "    cluster_labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    if(verbose):\n",
    "        print(f'numb clusters:{n_clusters},noise points: {n_noise}')\n",
    "    return hdbres(min_cluster_size,min_samples,n_clusters,n_noise,cluster_labels)\n",
    "\n",
    "#chack all combonations of following parameters\n",
    "cluster_sizes=[6,7,8,9,10,11,12,13,14,15]\n",
    "min_samps=[2,3,4]\n",
    "\n",
    "#holds hdbres objects\n",
    "res=[]\n",
    "for min_samp in min_samps:\n",
    "    for cluster_size in cluster_sizes:       \n",
    "          res.append(run_hdbscan(features_pca, cluster_size,min_samp,False))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(res, key=lambda x: x.n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the labels for a particulr run of interest\n",
    "# for instance try the one with self.min_cluster_size:10,min_samples:2\n",
    "cluster_labels=[x for x in res if x.min_cluster_size==10 and x.min_samples==2][0].labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the cluster predictions to orig df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the cluster predictions(np.array) to pandas dataframe\n",
    "dforig['Cluster']=cluster_labels.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the plots mean? If anything.\n",
    "You can only look at 3 features at a time if you are showing clusters as colors.<br>\n",
    "So look at a plot of all females, and then all males.  ARe any clusters exclusively female or male? <br>\n",
    "ARe there any other clusters that clearly indicate similarities?<br>\n",
    "Are there any that appear to be pointlessly mixed with other clusters?<br>\n",
    "If so should you try reducing the number of clusters calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only show non outliers\n",
    "show_pairplot(dforig[dforig['Cluster']!=-1],hue='Cluster');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like there is a relationship between Gender and age for cluster membership, lets plot\n",
    "# sns.barplot(data=dforig[dforig['Cluster']!=-1], y='Age',x='Gender',hue='Cluster');\n",
    "sns.barplot(data=dforig[dforig['Cluster']!=-1], y='spending_sale',x='Gender',hue='Cluster');\n",
    "# sns.barplot(data=dforig[dforig['Cluster']!=-1], y='spending_alcohol',x='Annual Income (k$)',hue='Cluster');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dforig.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is the problem, we should reduce the cardinality of every column except gender and Cluster\n",
    "#we will do it by binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
