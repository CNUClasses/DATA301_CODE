{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning - missing and duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Set max rows and columns displayed in jupyter\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a t-shirt order\n",
    "have a name, a t-shirt size, a t-shirt color and a weight(in pounds)<br>\n",
    "Uses the <a href=\"https://pypi.org/project/names/https://pypi.org/project/names/\">names </a> module to generate random names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut\n",
    "df = ut.generate_tshirt_order()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some duplicates just to show how to handle duplicates (delete them)\n",
    "#lets find the oldest male and female\n",
    "def fun1(df,numb=5):\n",
    "    #generates numb rows from df\n",
    "    return (df.iloc[0:numb,:])\n",
    "\n",
    "\n",
    "#generates numb rows from each group to be used as duplicates\n",
    "df_dups=df.groupby('t_shirt_size').apply(fun1)\n",
    "df_dups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append to original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,df_dups],ignore_index=True)\n",
    "#the old, soon to be deprecated way\n",
    "# df=df.append(df_dups, ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomly select 20% of rows for t_shirt_size ommision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first save orig size for later comparison\n",
    "df['t_shirt_size_orig'] = df['t_shirt_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "res = random.sample(range(0, len(df)), int(0.2 * len(df)))\n",
    "print(f'Number of rows to have \"t_shirt_size\" set to np.Nan is {len(res)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, save orig size\n",
    "# df['t_shirt_size_orig'] = df.loc[res,'t_shirt_size']\n",
    "\n",
    "#then lose orig size\n",
    "df.loc[res,'t_shirt_size']=np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many total\n",
    "#df.isna().sum().sum()\n",
    "\n",
    "#how many are null?\n",
    "df.t_shirt_size.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#show the missing data\n",
    "df[df.t_shirt_size.isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kind ='hist', hist will bin the number of weights and display them, hue will determine which color group they belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"weight\",  hue=\"t_shirt_size\", kind='hist', fill=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kind ='kde', kde is a kernel density estimater, essentially calculates a gaussian distribution around each point, and then adds these distributions, and then divides by the number of points to get the smooth curves you see that have an area of 1.  I'm showing it because its easier to see the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"weight\",  hue=\"t_shirt_size\", kind='kde', fill=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.t_shirt_size.value_counts()## What to do about duplicates?  Delete them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find them first\n",
    "start here 1/24/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually Verify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.duplicated(keep=False)].sort_values(by='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now drop the regular duplicates that are not missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop( df[df.duplicated(keep=False)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if one of the duplicated rows is missing the t-shirt size?  Then duplicated() will not find it.  Maybe we should check for duplicates in the 'name' column instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name.duplicated(keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that they are duplicates first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name.duplicated(keep=False)].sort_values(by='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to delete the one that has a np.nan for t_shirt_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the indexis that will be dropped\n",
    "# df[df.name.duplicated(keep=False) & (df.t_shirt_size.isna())].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop( df[df.name.duplicated(keep=False) & (df.t_shirt_size.isna())].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates are gone, now how to impute the missing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[(df['t_shirt_size'].isnull())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many match\n",
    "def printstats(df):\n",
    "    numbmatches=(df['t_shirt_size_orig']==df['t_shirt_size']).sum()\n",
    "    print(f'{numbmatches} tshirt sizes are correct out of {len(df)} total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One way is to use SimpleImputer and assign the median value to all the missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_med=df.copy()\n",
    "df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.t_shirt_size.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.t_shirt_size.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent',add_indicator=True)  #works with strings\n",
    "imp = imp.fit(df_med[['t_shirt_size']])   #here is where it determines what the most frequent is\n",
    "df_med['t_shirt_size']=imp.transform(df_med[['t_shirt_size']])[:,0] #here is where the transform is applied \n",
    "# imp.transform(df_med[['t_shirt_size']])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printstats(df_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way is to find the mean weight for each t-shirt size, and then assign missing value t-shirt size based on weight\n",
    "For each NaN, assign t-shirt size to closest mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First calculate average weight for each t-shirt size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_better = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = df_better.groupby('t_shirt_size').weight.mean()\n",
    "avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many in each group\n",
    "df_better.groupby('t_shirt_size').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute the value, replace any NaNs, and add a 1 in a column to indicate that this value was imputed \n",
    "<mark>The indicater column will inform a ML algorithm that this value was imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map works on a column apply works on a row, which means we have access tothe entire row\n",
    "#can also return more than 1 value so that we can have an indicator value\n",
    "\n",
    "def func(row):\n",
    "    if row.t_shirt_size is np.NaN:\n",
    "        #which avgs.weight is this weight closest to?\n",
    "        \n",
    "        #get a list of differences between this weight and average weights\n",
    "        lst_vals = [abs(row.weight-val) for val in avgs]\n",
    "\n",
    "        #get the index of the minimum value\n",
    "        min_val = min(lst_vals)\n",
    "        min_index=lst_vals.index(min_val)\n",
    "\n",
    "        #return t_shirt_size corresponding to this index\n",
    "        return pd.Series([avgs.index[min_index],True],index=['t_shirt_size','t_shirt_size_indicator'])\n",
    "    #its not missing, return what's there\n",
    "    return pd.Series([row.t_shirt_size,False],index=['t_shirt_size','t_shirt_size_indicator'])\n",
    "# df_better['t_shirt_size_indicator']=False\n",
    "df_better[['t_shirt_size','t_shirt_size_indicator']]=df_better.apply(func, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printstats(df_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see which ones it got wrong, look at the distributions in above plots\n",
    "#it got them wrong because the weights were outliers\n",
    "df_better[(df_better['t_shirt_size_indicator'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Slide for the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(df, numb=5):\n",
    "    return (df.iloc[0:numb,:])\n",
    "    \n",
    "df.groupby('t_shirt_size', dropna=False).apply(func,numb=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
