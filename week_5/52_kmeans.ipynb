{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kmeans Clustering\n",
    "\n",
    "**KMeans runs quickly (O(n)) but does not handle elongated clusters or non linear data well AND you have to specify the number of clusters up front (how would you possibly know this?). AND it is vulnerable to outliers because it will insist on placing them in a cluster** <br>\n",
    "The following illustrates a sample kmeans implementation to show the algorithm in action. For practical problems, use the kmeans implementation in scikitlearn (see the bottom of this notebook for sample code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('default')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#want to filter the seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMB_SAMPLES=500\n",
    "K_CLUSTERS = 3\n",
    "CLUSTER_STD = 1.0  #increase from 1 to 3 to make clusters overlap\n",
    "\n",
    "# RANDOM_STATE=7  #7 generates bad clusters because the initial points cluster centers were chosen poorly \n",
    "RANDOM_STATE=999  # this will choose better for this problem (kmeans++ does a good job of solving this problem, see below)\n",
    "UNKNOWN =-1\n",
    "\n",
    "import random\n",
    "random.seed(RANDOM_STATE)\n",
    "def get_random_centroids(X,k):\n",
    "    '''\n",
    "    chooses k centroids randomly from\n",
    "    the number of points in X\n",
    "    '''\n",
    "    samps=X.sample(n=k, random_state=RANDOM_STATE) #choose 3 random points as initial centroids\n",
    "    samps.reset_index(drop=True, inplace=True)  #replace orig index numbers with sequential ones\n",
    "    samps.drop(columns=['old_cluster_guess'], inplace=True) \n",
    "    samps.rename(columns={'cluster_guess':'cluster_number'}, inplace=True)\n",
    "    samps['cluster_number']=samps.index #assign cluster number to each centroid\n",
    "    return samps\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "def get_furthest_centroids(X,k):\n",
    "    '''\n",
    "    chooses k centroids by choosing the furthest\n",
    "    points from each other in X\n",
    "    '''\n",
    "    # Calculate the pairwise distances between all points\n",
    "    distances = pdist(X[['X0', 'X1']], metric='euclidean')\n",
    "    distance_matrix = squareform(distances)\n",
    "\n",
    "    # Find the indices of the 3 points that are furthest away from each other\n",
    "    furthest_points_indices = np.unravel_index(np.argsort(distance_matrix, axis=None)[-3:], distance_matrix.shape)\n",
    "\n",
    "    # Get the unique indices of the furthest points\n",
    "    unique_indices = np.unique(furthest_points_indices)\n",
    "    \n",
    "    # Get the furthest points\n",
    "    furthest_points = X.iloc[unique_indices].copy()\n",
    "    furthest_points.reset_index(drop=True, inplace=True)\n",
    "    furthest_points.drop(columns=['old_cluster_guess'], inplace=True) \n",
    "    furthest_points.rename(columns={'cluster_guess':'cluster_number'}, inplace=True)\n",
    "    furthest_points['cluster_number']=furthest_points.index #assign cluster number to each centroid\n",
    "    return furthest_points\n",
    "\n",
    "\n",
    "def plot_clusters(X, centroids=None, figsize=(5,3)):\n",
    "    '''\n",
    "    plots the data we are trying to cluster and the centroids\n",
    "    X: points to cluster (x,y)\n",
    "    centroids: centers of each cluster\n",
    "    '''\n",
    "    fig = plt.figure(figsize=figsize);\n",
    "    #notice that I'm plotting up to 3 scatterplots on the same figure\n",
    "    changedX=X[X.old_cluster_guess!=X.cluster_guess]  #how many changed since last time\n",
    "    if(len(changedX)>0):\n",
    "        sns.scatterplot(data=X[X.old_cluster_guess!=X.cluster_guess],x=\"X0\",y=\"X1\",hue='old_cluster_guess',s=40, legend=False, palette='Accent');\n",
    "    sns.scatterplot(data=X,x=\"X0\",y=\"X1\",hue='cluster_guess',s=10, legend=False, palette='Accent');\n",
    "    if(centroids is not None):\n",
    "        sns.scatterplot(data=centroids,x=\"X0\",y=\"X1\",s =200,hue='cluster_number', palette='Accent',legend=False);\n",
    "\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "from sklearn.datasets import make_blobs\n",
    "def generate_sample_data():\n",
    "    #Generate some clusters\n",
    "    #note that y denotes group membership, something we are trying to predict\n",
    "    X,y=make_blobs(n_samples=NUMB_SAMPLES,\n",
    "                n_features=2,\n",
    "                centers=K_CLUSTERS,\n",
    "                cluster_std=CLUSTER_STD,  #1 gives distinct clusters, 4 will give overlapping clusters\n",
    "                shuffle=True,\n",
    "                random_state=RANDOM_STATE)\n",
    "\n",
    "    #place in DataFrame\n",
    "    X=pd.DataFrame(data=X, columns=[\"X0\",\"X1\"])\n",
    "    X['cluster_guess']=UNKNOWN\n",
    "    X['old_cluster_guess']=UNKNOWN\n",
    "    return X\n",
    "import math\n",
    "def find_closest_cluster(ser, centroids):\n",
    "    # which cluster is this series point closest to\n",
    "    dsts = []  \n",
    "    for _,c in centroids.iterrows():\n",
    "        dsts.append(math.dist( (ser[0],ser[1]),(c[0],c[1])))\n",
    "\n",
    "    #which centroid\n",
    "    return (dsts.index(min(dsts)))\n",
    "\n",
    "def find_mean(df):\n",
    "    #find the mean\n",
    "    return df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate some data \n",
    "generate a synthetic dataset using sklearns makeblobs , see <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\">Make Blobs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=generate_sample_data()\n",
    "\n",
    "#want to see them?\n",
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means, General Algorithm\n",
    "<ol>\n",
    "    <li>Randomly pick *k* centroids from sample points as initial cluster centers</li>\n",
    "    <li>Assign each sample to the nearest centroid</li>\n",
    "    <li>Move the centroids to the center of the samples assigned to it</li>\n",
    "    <li>Repeat steps 2 and 3 until cluster membership stops changing</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick *k* centroids from sample points as initial cluster centers\n",
    "\n",
    "Either randomly, or the kmeans++ way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random \n",
    "# centroids=get_random_centroids(X,K_CLUSTERS)\n",
    "\n",
    "#kmeans++ ish (find points furthest away from each other)\n",
    "centroids=get_furthest_centroids(X,K_CLUSTERS)\n",
    "plot_clusters(X, centroids)\n",
    "# centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Kmeans converging - Repeat this cell until no points change clusters\n",
    "\n",
    "Assign each sample to the nearest centroid then move the centroids to the mean of their assigned clusters\n",
    "<mark>NOTE Points that changed will have their old centroid color as an outline and their new centroid color in the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign points to a cluster\n",
    "X['old_cluster_guess']=X['cluster_guess']\n",
    "X['cluster_guess']=X.apply(find_closest_cluster, centroids=centroids, axis=1)  #assign points to closest cluster\n",
    "\n",
    "new_centroid_center = X.groupby('cluster_guess').apply(find_mean)   #get new cluster mean\n",
    "centroids['X0']=new_centroid_center['X0']\n",
    "centroids['X1']=new_centroid_center['X1']\n",
    "\n",
    "numb_points_changed_clusters=sum(X.old_cluster_guess!=X.cluster_guess)  \n",
    "print(f'{numb_points_changed_clusters} points changed clusters')\n",
    "plot_clusters(X, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearns kmeans implementation\n",
    "Find sample code <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">here</a><br>\n",
    "<mark>Prefer to initialize the kmeans algorithm with k-means++, this puts the initial cluster center choices as far away from each other as possible which increases the chances for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a copy of original data\n",
    "X_cpy =generate_sample_data()\n",
    "\n",
    "#we need a numpy array for sklearns k_means\n",
    "Xnp=X_cpy.loc[:,['X0','X1']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# initialize with k-means++, this puts the initial cluster choices as far away from each other\n",
    "# as possible which increases the chances for convergence\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=RANDOM_STATE)\n",
    "kmeans=kmeans.fit(Xnp)  #iteratively calculates the clusters\n",
    "\n",
    "#lets see what we have\n",
    "X_cpy['cluster_guess']=pd.Series(kmeans.labels_)\n",
    "X_cpy['old_cluster_guess']=pd.Series(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame(kmeans.cluster_centers_, columns=['X0','X1'])\n",
    "a['cluster_number']=a.index\n",
    "\n",
    "plot_clusters(X_cpy, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once fitted you can predict what clusters new data would belong too\n",
    "kmeans.predict([[0,0],[-7.5,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and see what clusters all the fitted data belong to\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where are the cluster centers?\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert cluster centers to dataframe to use the above plotting function\n",
    "centroids1=pd.DataFrame(data=kmeans.cluster_centers_, columns=['X0', 'X1'])\n",
    "centroids1.reset_index( inplace=True)\n",
    "centroids1.rename(columns={'index':'cluster_number'}, inplace=True)\n",
    "centroids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X_cpy, centroids1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inertia: For each cluster, square the sum of the distance between every point in that particular cluster and the cluster centroid.  Add these cluster sums together. \n",
    "![](./SSEKmeans.png)\n",
    "\n",
    "What good is it? It helps you calculate the correct number of clusters for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the inertia for the above sklearn implementation\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The elbow method, used to find optimal number of clusters for kmeans\n",
    "Calculate the inertia for different number of clusters.  As the number of clusters increase the inertia will decrease ( more clusters means each cluster is smaller so the SSE will also be smaller given that the points will also be closer to cluster centers)<br>\n",
    "The idea is to identify the value of K where the inertia starts to rapidly increase. This means that the clusters are aquiring member points that are further and further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First  get the inertias for various K's\n",
    "inertias=[]\n",
    "for k in range (1,11):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=RANDOM_STATE).fit(Xnp)\n",
    "    inertias.append(kmeans.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then plot them\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot( range(1,11), inertias, marker='o')\n",
    "# ax.annotate('looks OK', xy=(3, 800), xytext=(4, 1500),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.xlabel=('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia starts to rapidly increase from 3 to 2 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately the elbow method (and following silhouette plots) are often ambiguous when the clusters are not linearly seperable\n",
    "\n",
    "<mark>Increase the CLUSTER_STD param in second cell from 1.0 to 3.0 then rerun the elbow method above. make_blobs (in function generate_sample_data) will create clusters that are no longer linearly seperable which makes the elbow method hard to interpret.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Plots, a way to visually evaluate the quality of clustering (works with other cluster methods as well)\n",
    "Works with all distance based clustering methods,  see the reference on class website titled 'Selecting the number of clusters with silhouette analysis' for explanation and use\n",
    "\n",
    "Steps to calculate silhouette score\n",
    "\n",
    "1. For each data point, the average distance (a_i) to other data points within the same cluster is calculated. This value represents the similarity level of the data point to others in its cluster.\n",
    "\n",
    "2. For each data point, the average distance (b_i) to all other clusters it doesn’t belong to is computed. This value indicates how different the data point is from data points in other clusters.\n",
    "\n",
    "3. The Silhouette score is calculated using the formula:\n",
    "\n",
    "Silhouette Score = (b_i — a_i) / max(a_i, b_i)\n",
    "\n",
    "4. By taking the average of the Silhouette scores calculated for each data point, an overall Silhouette score is obtained, which measures the success of clustering results.\n",
    "\n",
    "Key characteristics of the Silhouette score include:<mark>\n",
    "It ranges from -1 to +1:</mark>\n",
    "\n",
    "Positive values indicate that data points belong to the correct clusters, indicating good clustering results.\n",
    "\n",
    "A score of zero suggests overlapping clusters or data points equally close to multiple clusters.\n",
    "\n",
    "Negative values indicate that data points are assigned to incorrect clusters, indicating poor clustering results.\n",
    "\n",
    "A higher Silhouette score indicates better clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average silhoutte for various K's (calculated by averaging each points score)\n",
    "silhouette_avgs=[]\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=RANDOM_STATE).fit(Xnp)\n",
    "    print(f\"for {k} clusters, silhouette_score={silhouette_score(Xnp, list(kmeans.labels_))}\")\n",
    "    # silhouette_avgs.append(silhouette_score(Xnp, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
